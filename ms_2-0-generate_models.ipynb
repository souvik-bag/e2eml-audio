{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddb56f62",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     WhisperProcessor, WhisperForConditionalGeneration,\n\u001b[0;32m      8\u001b[0m     Speech2TextProcessor, Speech2TextForConditionalGeneration\n\u001b[0;32m      9\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ziyan\\anaconda3\\envs\\whisper_export\\lib\\site-packages\\transformers\\__init__.py:958\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m    956\u001b[0m _import_structure \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mset\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _import_structure\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m--> 958\u001b[0m import_structure \u001b[38;5;241m=\u001b[39m \u001b[43mdefine_import_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    959\u001b[0m import_structure[\u001b[38;5;28mfrozenset\u001b[39m({})]\u001b[38;5;241m.\u001b[39mupdate(_import_structure)\n\u001b[0;32m    961\u001b[0m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m _LazyModule(\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    966\u001b[0m     extra_objects\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m: __version__},\n\u001b[0;32m    967\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ziyan\\anaconda3\\envs\\whisper_export\\lib\\site-packages\\transformers\\utils\\import_utils.py:2867\u001b[0m, in \u001b[0;36mdefine_import_structure\u001b[1;34m(module_path, prefix)\u001b[0m\n\u001b[0;32m   2843\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m\n\u001b[0;32m   2844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefine_import_structure\u001b[39m(module_path: \u001b[38;5;28mstr\u001b[39m, prefix: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m IMPORT_STRUCTURE_T:\n\u001b[0;32m   2845\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2846\u001b[0m \u001b[38;5;124;03m    This method takes a module_path as input and creates an import structure digestible by a _LazyModule.\u001b[39;00m\n\u001b[0;32m   2847\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2865\u001b[0m \u001b[38;5;124;03m    If `prefix` is not None, it will add that prefix to all keys in the returned dict.\u001b[39;00m\n\u001b[0;32m   2866\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2867\u001b[0m     import_structure \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2868\u001b[0m     spread_dict \u001b[38;5;241m=\u001b[39m spread_import_structure(import_structure)\n\u001b[0;32m   2870\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ziyan\\anaconda3\\envs\\whisper_export\\lib\\site-packages\\transformers\\utils\\import_utils.py:2580\u001b[0m, in \u001b[0;36mcreate_import_structure_from_path\u001b[1;34m(module_path)\u001b[0m\n\u001b[0;32m   2578\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(module_path):\n\u001b[0;32m   2579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__pycache__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(module_path, f)):\n\u001b[1;32m-> 2580\u001b[0m         import_structure[f] \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2582\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, f)):\n\u001b[0;32m   2583\u001b[0m         adjacent_modules\u001b[38;5;241m.\u001b[39mappend(f)\n",
      "File \u001b[1;32mc:\\Users\\ziyan\\anaconda3\\envs\\whisper_export\\lib\\site-packages\\transformers\\utils\\import_utils.py:2605\u001b[0m, in \u001b[0;36mcreate_import_structure_from_path\u001b[1;34m(module_path)\u001b[0m\n\u001b[0;32m   2602\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   2604\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, module_name), encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m-> 2605\u001b[0m     file_content \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2607\u001b[0m \u001b[38;5;66;03m# Remove the .py suffix\u001b[39;00m\n\u001b[0;32m   2608\u001b[0m module_name \u001b[38;5;241m=\u001b[39m module_name[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ziyan\\anaconda3\\envs\\whisper_export\\lib\\codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_buffer_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, errors, final):\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;66;03m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m    322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_decode(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors, final)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join as pjoin, exists as pexists\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    WhisperProcessor, WhisperForConditionalGeneration,\n",
    "    Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2b6447",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './ms_2'\n",
    "models_root = pjoin(root, 'models')\n",
    "os.makedirs(models_root, exist_ok=True)\n",
    "\n",
    "a2t_model_names = [\n",
    "    # \"AventIQ-AI/whisper-audio-to-text\",\n",
    "    # \"facebook/s2t-small-librispeech-asr\"\n",
    "\n",
    "    # \"facebook/wav2vec2-base-960h\",\n",
    "    # \"superb/hubert-base-superb-asr\",\n",
    "    # \"microsoft/wavlm-base-plus\",\n",
    "    \n",
    "    \"facebook/wav2vec2-base-960h\",\n",
    "    # \"facebook/s2t-small-librispeech-asr\",\n",
    "]\n",
    "\n",
    "t2s_model_names = [\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"siebert/sentiment-roberta-large-english\",\n",
    "    \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f3d3d7",
   "metadata": {},
   "source": [
    "### Export audio to text models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7705210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: Saved model (fp32) facebook/wav2vec2-base-960h at ./ms_2\\models/a2t#facebook_wav2vec2-base-960h#fp32.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ziyan\\anaconda3\\envs\\whisper_export\\lib\\site-packages\\onnxconverter_common\\float16.py:70: UserWarning: the float32 number -5.960464477539063e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\n",
      "c:\\Users\\ziyan\\anaconda3\\envs\\whisper_export\\lib\\site-packages\\onnxconverter_common\\float16.py:52: UserWarning: the float32 number 5.960464477539063e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\n",
      "c:\\Users\\ziyan\\anaconda3\\envs\\whisper_export\\lib\\site-packages\\onnxconverter_common\\float16.py:52: UserWarning: the float32 number 7.551238523362258e-10 will be truncated to 1e-07\n",
      "  warnings.warn(\n",
      "c:\\Users\\ziyan\\anaconda3\\envs\\whisper_export\\lib\\site-packages\\onnxconverter_common\\float16.py:70: UserWarning: the float32 number -9.835351422182725e-10 will be truncated to -1e-07\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: Saved model (fp16) facebook/wav2vec2-base-960h at ./ms_2\\models/t2s#facebook_wav2vec2-base-960h#fp16.onnx\n",
      "DONE: Saved model (int8) facebook/wav2vec2-base-960h at ./ms_2\\models/t2s#facebook_wav2vec2-base-960h#int8.onnx\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "from onnxconverter_common import float16\n",
    "import onnx\n",
    "\n",
    "import torch, os\n",
    "\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "dummy = torch.randn(1, 16000)  # 1s of 16kHz audio\n",
    "\n",
    "out_p = f\"{models_root}/a2t_{model_name.replace('/', '_')}_fp32.onnx\"\n",
    "\n",
    "if not pexists(out_p):\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy,\n",
    "        out_p,\n",
    "        input_names=[\"input_values\"],\n",
    "        output_names=[\"logits\"],\n",
    "        dynamic_axes={\"input_values\": {0: \"batch\", 1: \"sequence\"}},\n",
    "        opset_version=14\n",
    "    )\n",
    "\n",
    "print(f\"DONE: Saved model (fp32) {model_name} at {out_p}\")\n",
    "\n",
    "out_path_fp16 = f\"{models_root}/t2s_{model_name.replace('/', '_')}_fp16.onnx\"\n",
    "if not pexists(out_path_fp16):\n",
    "    model_fp32 = onnx.load(out_p)\n",
    "    model_fp16 = float16.convert_float_to_float16(model_fp32)\n",
    "    onnx.save(model_fp16, out_path_fp16)\n",
    "print(f\"DONE: Saved model (fp16) {model_name} at {out_path_fp16}\")\n",
    "\n",
    "\n",
    "out_path_int8 = f\"{models_root}/t2s_{model_name.replace('/', '_')}_int8.onnx\"\n",
    "if not pexists(out_path_int8):\n",
    "    quantize_dynamic(\n",
    "        out_p,\n",
    "        out_path_int8,\n",
    "        weight_type=QuantType.QUInt8,\n",
    "    )\n",
    "print(f\"DONE: Saved model (int8) {model_name} at {out_path_int8}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a7dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: Saved model facebook/s2t-small-librispeech-asr at ./ms_2\\models/a2t#superb_hubert-base-superb-asr#fp32.onnx\n"
     ]
    }
   ],
   "source": [
    "from optimum.exporters.onnx import main_export\n",
    "from pathlib import Path\n",
    "\n",
    "model_name = \"facebook/s2t-small-librispeech-asr\"\n",
    "output_dir = pjoin(f\"{models_root}/a2t_{model_name.replace('/', '_')}_fp32\")\n",
    "\n",
    "if not pexists(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=False)\n",
    "    main_export(\n",
    "        model_name_or_path=model_name,\n",
    "        output=output_dir,\n",
    "        task=\"automatic-speech-recognition\",\n",
    "    )\n",
    "print(f\"DONE: Saved model {model_name} at {out_p}\")\n",
    "\n",
    "# from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# for quant in ['']\n",
    "# quantize_dir = pjoin(f\"{models_root}/a2t_{model_name.replace('/', '_')}_int8\")\n",
    "# if not pexists(quantize_dir):\n",
    "#     shutil.copytree(output_dir, quantize_dir)\n",
    "#     quantize_dynamic(\n",
    "#         f\"{quantize_dir}/encoder_model.onnx\",\n",
    "#         f\"{quantize_dir}/encoder_model.onnx\",\n",
    "#         weight_type=QuantType.QUInt8\n",
    "#     )\n",
    "    \n",
    "#     quantize_dynamic(\n",
    "#         f\"{quantize_dir}/decoder_model.onnx\",\n",
    "#         f\"{quantize_dir}/decoder_model.onnx\",\n",
    "#         weight_type=QuantType.QUInt8\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbc7d4c",
   "metadata": {},
   "source": [
    "### Export text to sentiment models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b175c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: Saved model (fp32) distilbert-base-uncased-finetuned-sst-2-english at ./ms_2\\models/a2t#facebook_wav2vec2-base-960h#fp32.onnx\n",
      "DONE: Saved model (fp16) distilbert-base-uncased-finetuned-sst-2-english at ./ms_2\\models/t2s#distilbert-base-uncased-finetuned-sst-2-english#fp16.onnx\n",
      "DONE: Saved model (int8) distilbert-base-uncased-finetuned-sst-2-english at ./ms_2\\models/t2s#distilbert-base-uncased-finetuned-sst-2-english#int8.onnx\n",
      "DONE: Saved model (fp32) siebert/sentiment-roberta-large-english at ./ms_2\\models/a2t#facebook_wav2vec2-base-960h#fp32.onnx\n",
      "DONE: Saved model (fp16) siebert/sentiment-roberta-large-english at ./ms_2\\models/t2s#siebert_sentiment-roberta-large-english#fp16.onnx\n",
      "DONE: Saved model (int8) siebert/sentiment-roberta-large-english at ./ms_2\\models/t2s#siebert_sentiment-roberta-large-english#int8.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: Saved model (fp32) cardiffnlp/twitter-roberta-base-sentiment-latest at ./ms_2\\models/a2t#facebook_wav2vec2-base-960h#fp32.onnx\n",
      "DONE: Saved model (fp16) cardiffnlp/twitter-roberta-base-sentiment-latest at ./ms_2\\models/t2s#cardiffnlp_twitter-roberta-base-sentiment-latest#fp16.onnx\n",
      "DONE: Saved model (int8) cardiffnlp/twitter-roberta-base-sentiment-latest at ./ms_2\\models/t2s#cardiffnlp_twitter-roberta-base-sentiment-latest#int8.onnx\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "from onnxconverter_common import float16\n",
    "import onnx\n",
    "\n",
    "def export_model(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    dummy_inputs = tokenizer(\"I love this movie!\", return_tensors=\"pt\")\n",
    "\n",
    "    out_path = f\"{models_root}/t2s_{model_name.replace('/', '_')}_fp32.onnx\"\n",
    "\n",
    "    if not pexists(out_path):\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            (dummy_inputs[\"input_ids\"], dummy_inputs[\"attention_mask\"]),\n",
    "            out_path,\n",
    "            input_names=[\"input_ids\", \"attention_mask\"],\n",
    "            output_names=[\"logits\"],\n",
    "            opset_version=14,\n",
    "            dynamic_axes={\n",
    "                \"input_ids\": {0: \"batch_size\", 1: \"sequence\"},\n",
    "                \"attention_mask\": {0: \"batch_size\", 1: \"sequence\"},\n",
    "            },\n",
    "        )\n",
    "    print(f\"DONE: Saved model (fp32) {model_name} at {out_p}\")\n",
    "\n",
    "    \n",
    "    out_path_fp16 = f\"{models_root}/t2s_{model_name.replace('/', '_')}_fp16.onnx\"\n",
    "    if not pexists(out_path_fp16):\n",
    "        model_fp32 = onnx.load(out_path)\n",
    "        model_fp16 = float16.convert_float_to_float16(model_fp32)\n",
    "        onnx.save(model_fp16, out_path_fp16)\n",
    "    print(f\"DONE: Saved model (fp16) {model_name} at {out_path_fp16}\")\n",
    "\n",
    "\n",
    "    out_path_int8 = f\"{models_root}/t2s_{model_name.replace('/', '_')}_int8.onnx\"\n",
    "    if not pexists(out_path_int8):\n",
    "        quantize_dynamic(\n",
    "            out_path,\n",
    "            out_path_int8,\n",
    "            weight_type=QuantType.QUInt8,\n",
    "        )\n",
    "    print(f\"DONE: Saved model (int8) {model_name} at {out_path_int8}\")\n",
    "\n",
    "\n",
    "for m in t2s_model_names:\n",
    "    export_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23a293ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input name: input_ids, shape: ['batch_size', 'sequence'], type: tensor(int64)\n",
      "Input name: attention_mask, shape: ['batch_size', 'sequence'], type: tensor(int64)\n",
      "{'NEGATIVE': 0.0005933608626946807, 'POSITIVE': 0.9994066953659058}\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 1️⃣ 加载 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# 2️⃣ 载入 onnx 模型\n",
    "t2s_model_p = \"./ms_2/models/t2s#distilbert-base-uncased-finetuned-sst-2-english#int8.onnx\"\n",
    "session = ort.InferenceSession(t2s_model_p)\n",
    "\n",
    "# 3️⃣ 文本转输入张量\n",
    "text = \"I love how smooth this app runs!\"\n",
    "inputs = tokenizer(text, return_tensors=\"np\", padding=True, truncation=True)\n",
    "\n",
    "# 查看模型输入名称\n",
    "for inp in session.get_inputs():\n",
    "    print(f\"Input name: {inp.name}, shape: {inp.shape}, type: {inp.type}\")\n",
    "\n",
    "# 4️⃣ 取输入名称\n",
    "input_names = [i.name for i in session.get_inputs()]\n",
    "output_names = [o.name for o in session.get_outputs()]\n",
    "\n",
    "# 5️⃣ 执行推理\n",
    "outputs = session.run(output_names, {\n",
    "    input_names[0]: inputs[\"input_ids\"].astype(np.int64),\n",
    "    input_names[1]: inputs[\"attention_mask\"].astype(np.int64)\n",
    "})\n",
    "\n",
    "# 6️⃣ softmax 得出概率\n",
    "logits = torch.tensor(outputs[0])\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "labels = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "\n",
    "print({labels[i]: float(probs[0][i]) for i in range(2)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d16c9e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AventIQ-AI/whisper-audio-to-text is saved into ./ms_2/models/a2t_AventIQ-AI_whisper-audio-to-text_fp32\n",
      "Saved model type: torch.float32\n",
      "AventIQ-AI/whisper-audio-to-text is saved into ./ms_2/models/a2t_AventIQ-AI_whisper-audio-to-text_fp16\n",
      "Saved model type: torch.float16\n",
      "AventIQ-AI/whisper-audio-to-text is saved into ./ms_2/models/a2t_AventIQ-AI_whisper-audio-to-text_int8\n",
      "Saved model type: torch.float16\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "model_name = \"AventIQ-AI/whisper-audio-to-text\"\n",
    "model_dir = f\"./ms_2/models/a2t_{model_name.replace('/', '_')}_fp32\"\n",
    "\n",
    "if not pexists(model_dir):\n",
    "    snapshot_download(\n",
    "        repo_id=\"AventIQ-AI/whisper-audio-to-text\",\n",
    "        local_dir=model_dir,\n",
    "        resume_download=True\n",
    "    )\n",
    "print(f\"{model_name} is saved into {model_dir}\")\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_dir)\n",
    "processor = WhisperProcessor.from_pretrained(model_dir)\n",
    "print(f'Saved model type: {next(model.parameters()).dtype}')\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "model_fp16 = model.half()\n",
    "model_dir_fp16 = f\"./ms_2/models/a2t_{model_name.replace('/', '_')}_fp16\"\n",
    "if not pexists(model_dir_fp16):\n",
    "    model_fp16.save_pretrained(model_dir_fp16)\n",
    "    processor.save_pretrained(model_dir_fp16)\n",
    "print(f\"{model_name} is saved into {model_dir_fp16}\")\n",
    "print(f'Saved model type: {next(model_fp16.parameters()).dtype}')\n",
    "\n",
    "\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "model_int8 = torch.quantization.quantize_dynamic(model, \n",
    "                                                 {torch.nn.Linear}, \n",
    "                                                 dtype=torch.qint8)\n",
    "\n",
    "model_dir_int8 = f\"./ms_2/models/a2t_{model_name.replace('/', '_')}_int8\"\n",
    "if not pexists(model_dir_int8):\n",
    "    os.makedirs(model_dir_int8, exist_ok=True)\n",
    "    torch.save(model_int8.state_dict(), f\"{model_dir_int8}/pytorch_model.bin\")\n",
    "    processor.save_pretrained(model_dir_int8)\n",
    "    model.config.save_pretrained(model_dir_int8)\n",
    "print(f\"{model_name} is saved into {model_dir_int8}\")\n",
    "print(f'Saved model type: {next(model_int8.parameters()).dtype}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper_export",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
